nohup: ignoring input
Using strategy: PrecisionStrategy(name='FP16_compute_FP32_accum', input_dtype=torch.float16, weight_dtype=torch.float16, compute_dtype=torch.float16, output_dtype=torch.float16)

===== 批次 1/200 =====
[worker 3] start. device_id=3, use_cuda=True, device=cuda:3
[worker 3] torch.cuda.is_available()=True, cuda_count=4
[worker 3] device name: Tesla V100-SXM2-32GB
[worker 3] memory_allocated(before) = 0
[worker 3] memory_allocated(after) = 78,137,344
[worker 3] max_memory_allocated = 128,604,160
[worker 3] finished: total_worker_time=0.6143s, generated 128 errors
[worker 2] start. device_id=2, use_cuda=True, device=cuda:2
[worker 2] torch.cuda.is_available()=True, cuda_count=4
[worker 2] device name: Tesla V100-SXM2-32GB
[worker 2] memory_allocated(before) = 0
[worker 2] memory_allocated(after) = 78,137,344
[worker 2] max_memory_allocated = 128,604,160
[worker 2] finished: total_worker_time=0.6600s, generated 128 errors
[worker 0] start. device_id=0, use_cuda=True, device=cuda:0
[worker 0] torch.cuda.is_available()=True, cuda_count=4
[worker 0] device name: Tesla V100-SXM2-32GB
[worker 0] memory_allocated(before) = 0
[worker 0] memory_allocated(after) = 78,137,344
[worker 0] max_memory_allocated = 128,604,160
[worker 0] finished: total_worker_time=0.6688s, generated 128 errors
[worker 1] start. device_id=1, use_cuda=True, device=cuda:1
[worker 1] torch.cuda.is_available()=True, cuda_count=4
[worker 1] device name: Tesla V100-SXM2-32GB
[worker 1] memory_allocated(before) = 0
[worker 1] memory_allocated(after) = 78,137,344
[worker 1] max_memory_allocated = 128,604,160
[worker 1] finished: total_worker_time=0.7127s, generated 128 errors
Traceback (most recent call last):
  File "/root/anaconda3/envs/precision-guard/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/anaconda3/envs/precision-guard/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/root/precision_estimation/examples/conv2d_precision_pipeline.py", line 167, in <module>
    main()
  File "/root/precision_estimation/examples/conv2d_precision_pipeline.py", line 67, in main
    exceeded, actual_err, bound, oracle_result = detector.detect(x, w)
  File "/root/anaconda3/envs/precision-guard/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/root/precision_estimation/core/detector/conv2d_error_detector.py", line 40, in detect
    oracle_result: OracleResult = self.oracle.predict_error_bound(x, w)
  File "/root/precision_estimation/core/oracle/conv2d_oracle_mc.py", line 350, in predict_error_bound
    comp = self._estimate_components(x_cpu, w_cpu, num_samples=min(128, max(8, self.num_mc_samples // 8)))
  File "/root/precision_estimation/core/oracle/conv2d_oracle_mc.py", line 399, in _estimate_components
    "input_storage_error": run((True, False, False, False)),
  File "/root/precision_estimation/core/oracle/conv2d_oracle_mc.py", line 392, in run
    raise RuntimeError(f"Component worker error: {err_msg}")
RuntimeError: Component worker error: RuntimeError('Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same')
Traceback (most recent call last):
  File "/root/precision_estimation/core/oracle/conv2d_oracle_mc.py", line 211, in _worker_run
    y_c = F.conv2d(
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same

[worker 0] start. device_id=0, use_cuda=True, device=cuda:0
[worker 0] torch.cuda.is_available()=True, cuda_count=4
[worker 0] device name: Tesla V100-SXM2-32GB
[worker 0] memory_allocated(before) = 0
